"""
Todo:
    * switch to OpenCV plotting backend
    * setup automated Ontology from reference image or using a CLIP model (https://docs.autodistill.com/utilities/use-embeddings-in-classification/#code-reference)
"""

import os
import glob
import shutil
import enum
import matplotlib
import matplotlib.pyplot as plt
from typing import Dict, List, Optional, Tuple
from pathlib import Path
from argparse import ArgumentParser

from tqdm import tqdm
import shelve
import tempfile
import importlib
import cv2
import supervision as sv
from supervision.dataset.formats.yolo import save_yolo_annotations, detections_to_yolo_annotations
from supervision.dataset.core import DetectionDataset
from autodistill.detection import CaptionOntology, DetectionOntology
from autodistill.utils import plot
from ultralytics import YOLO
from autodistill_yolov8 import YOLOv8

import albumentations as A

import roboflow
from autodistill.core import BaseModel
from autodistill.helpers import load_image, split_data
# matplotlib.use('Qt5Agg')


def parse_args():
    parser = ArgumentParser(
            description='Script for automatically labelling images, videos and directories, '
                        'then annotating, optionally augmenting and finally training a target model.')
    parser.add_argument('--run-mode', default='training', choices=['training', 'inference'],
                        help='The mode to run the script in. Either training, or inference (on an image or video).')

    parser.add_argument('--captions-dict', default='',
                        help='The dictionary of captions to send to the base model. '
                             'Currently unimplemented via command line. Modify the script instead.')

    parser.add_argument('--base-model', default='grounded_sam2',
                        help='The large teacher model to use for annotation/prediction. '
                             'Recommended: grounded_sam2 for segmentaion, grounding_dino for detection.',
                        choices=['grounding_dino', 'grounded_sam', 'grounded_sam2',
                                 'detic', 'florence_2', 'yolov8', 'owlv2', 'codet'], required=True)

    parser.add_argument('--target-model', default='yolov8',
                        help='Smaller student model to train.',
                        choices=['yolov8', 'yolov5', 'yolov6', 'yolo-nas', 'yolov7', 'detr'])

    parser.add_argument('--target-model-weights', default='yolov8x.pt',
                        help='The starting weight to use for training the target model.')

    parser.add_argument(
            '--extract-frames-from-videos', default=True, type=bool,
            help='Whether to extract video frames from the "--video-dir" directory to the "--frame-dir" directory.')

    parser.add_argument(
            '--remove-duplicates', default=True, type=bool,
            help='Whether to remove duplicate labels using NMS. Recommended True.')
    parser.add_argument(
            '--iou-threshold', default=0.85, type=float,
            help='IoU threshold to consider as overlap.')
    parser.add_argument(
            '--class-agnostic', default=False, type=bool,
            help='Whether to consider class labels when removing duplicates. False means to consider classes.')

    parser.add_argument(
            '--augment-dataset', default=False, help='')

    parser.add_argument(
            '--save-captions', type=bool, default=True, help='Whether to save the '
                                                             'captions/prompts/ontology used by the base model in '
                                                             'generating the labels. Recommended True.')

    parser.add_argument('--show', default=True, help='Whether to display the inference video.')
    parser.add_argument('--save-inference-results', type=bool, default=False,
                        help='Whether to save the results when running in inference mode.')
    parser.add_argument('--save-auto-labels-video', type=bool, default=False,
                        help='Whether to save a video showing the labels generated by the base model.')
    parser.add_argument('--save-individual-frames', default=False,
                        help='Whether to save the predictions on individual frames.')

    parser.add_argument('--video-dir', default='./video_dir',
                        help='Path to video directory for extracting frames --extract-frames=True')

    parser.add_argument('--frame-dir', default='./frames',
                        help='Directory to save frames extracted. Also the directory used for automatic labelling.')

    parser.add_argument('--dataset-dir', default='./dataset',
                        help='Directory to save annotated labels and configs.')

    parser.add_argument('--inference-file', default='./frames/image1.png',
                        help='Path to a PNG or video file to run inference on if --run-mode=inference')

    parser.add_argument('--inference-save-path', default='./annotations/annotated_image1.png',
                        help='Path to save inference results if --save-inference-results=True')
    parser.add_argument('--interactive mode', default=False,
                        help='Whether to print current action and wait for input before proceeding to the next step.')
    args = parser.parse_args()
    return args


class NmsSetting(str, enum.Enum):
    NONE = "no_nms"
    CLASS_SPECIFIC = "class_specific"
    CLASS_AGNOSTIC = "class_agnostic"


class AutoLabeler(object):
    """docstring for ClassName"""

    def __init__(self,
                 mode='training',
                 base='grounding_dino',
                 base_model_weights='yolov5su.pt',  # "yolov5s.pt", "yolov8x.pt"
                 target='yolov8',
                 target_model_weights='yolov8x.pt',
                 captions_dict=None,
                 video_dir='',
                 frame_dir='',
                 dataset_dir='',
                 extract_frames_from_videos=False,
                 extraction_stride=1,
                 extraction_max_frames=-1,
                 save_labels=True,
                 label_file_size_threshold=1000,
                 label_batch_size=1000,
                 save_inference_results=False,
                 inference_file='',
                 inference_save_path='',
                 save_video=False,
                 save_captions=False,
                 show_label=True,
                 augment_dataset=False,
                 chain_augmentations=False,
                 remove_duplicates=True,
                 iou_threshold=0.85,
                 class_agnostic=False,
                 interactive_mode=False):
        """Constructor for AutoLabeler"""
        super(AutoLabeler, self).__init__()
        self.mode = mode  # training or inference
        self.base = base  # base model to load
        self.base_model_weights = base_model_weights  # path to base model
        self.target = target  # target model to save for inference
        self.target_model_weights = target_model_weights  # path to target model
        self.captions_dict = captions_dict  # caption
        self.video_dir = video_dir  # path to video directory for extracting frames
        self.frame_dir = frame_dir  # path to save frames
        self.dataset_dir = dataset_dir  # path to save labels and configs
        self.extract_frames_from_videos = extract_frames_from_videos  # whether to extract frames to the frame directory
        self.extraction_stride = extraction_stride  # stride for extracting frames
        self.extraction_max_frames = extraction_max_frames  # maximum number of frames to extract
        self.save_labels = save_labels  # whether to save labels to the dataset directory in either mode
        self.label_file_size_threshold = label_file_size_threshold  # file size threshold for saving labels
        self.label_batch_size = label_batch_size  # batch size for saving labels
        self.save_inference_results = save_inference_results  # whether to save the inference results
        self.inference_file = inference_file  # image or video file for 'inference' mode
        self.inference_save_path = inference_save_path  # path to save inference results
        self.save_video = save_video  # whether to save the video with labels when in 'inference' mode
        self.save_captions = save_captions  # whether to save the captions as a text file
        self.show_label = show_label  # whether to display inference or auto labelled results
        self.augment_dataset = augment_dataset  # whether to augment the dataset
        # whether to chain augmentations into a single image. False produces (n_images * n_augmentations) images
        self.chain_augmentations = chain_augmentations
        self.remove_duplicates = remove_duplicates
        self.iou_threshold = iou_threshold
        self.class_agnostic = class_agnostic
        self.interactive_mode = interactive_mode  # Whether to print the current action and wait for input before proceeding to the next step.

        self.dataset = None
        self.base_model = None
        self.target_model = None

        if self.captions_dict is None:
            self.captions_dict = {
                "person": "person",
                "car": "car"
            }

        self.ontology = CaptionOntology(self.captions_dict)

        try:
            grounded_sam_base = None
            grounded_sam_base = GroundedSAM(ontology=self.ontology)
        except NameError:
            pass

        try:
            grounded_sam2_base = None
            grounded_sam2_base = GroundedSAM2(ontology=self.ontology)
        except NameError:
            pass

        try:
            grounding_dino_base = None
            grounding_dino_base = GroundingDINO(ontology=self.ontology)
        except NameError:
            pass

        try:
            yolov8_base = None
            yolov8_base = YOLOv8Base(ontology=self.ontology, weights_path=self.base_model_weights)
        except NameError:
            pass

        try:
            owlvit_base = None
            owlvit_base = OWLViT(ontology=self.ontology)
        except NameError:
            pass

        try:
            detic_base = None
            detic_base = DETIC(ontology=self.ontology)
        except NameError:
            pass

        try:
            codet_base = None
            codet_base = CoDet(ontology=self.ontology)
        except NameError:
            pass

        try:
            owlv2_base = None
            owlv2_base = OWLv2(ontology=self.ontology)
        except NameError:
            pass

        try:
            florence2_base = None
            florence2_base = Florence2(ontology=self.ontology)
        except NameError:
            pass

        self.base_dict = {
            'grounding_dino': grounding_dino_base,
            'grounded_sam': grounded_sam_base,
            'yolov8': yolov8_base,
            'grounded_sam2': grounded_sam2_base,
            'owl_vit': owlvit_base,
            'detic': detic_base,
            'owlv2': owlv2_base,
            'codet': codet_base,
            'florence_2': florence2_base,
        }
        self.base_model = self.base_dict[self.base]

        if self.target == 'yolov8':
            self.target_dict = {
                'yolov8': YOLOv8
            }
            self.target_model = self.target_dict[self.target](self.target_model_weights)
        else:
            self.target_model = None

        self.augmentation_pipeline = None
        if self.augment_dataset:
            self.setup_augmentation_pipeline(chained=self.chain_augmentations)

    def update_caption(self, captions_dict):
        self.captions_dict = captions_dict
        self.ontology = CaptionOntology(self.captions_dict)
        self.base_model.ontology = self.ontology

    def extract_frames(self, video_dir=None, frame_dir=None, save_frames_in_separate_dir=False,
                       stride=1, max_frames=-1):
        if video_dir is None:
            video_dir = self.video_dir
        if frame_dir is None:
            frame_dir = self.frame_dir
        if (stride is None) or (stride < 1):
            stride = self.extraction_stride
        if max_frames < 0 or max_frames is None:
            max_frames = self.extraction_max_frames

        if not os.path.exists(frame_dir):
            os.makedirs(frame_dir)

        # check if video_dir is a single video or a directory of videos
        if os.path.isdir(video_dir):
            # video_files = [os.path.join(video_dir, f) for f in os.listdir(video_dir) if f.endswith('.mp4')]
            video_files = [f for f in os.listdir(video_dir) if f.endswith('.mp4')]
        else:
            video_files = [video_dir]

        num_frames_saved = 0
        for video_file in video_files:
            video_path = os.path.join(video_dir, video_file)
            video_file_name = os.path.splitext(video_file)[0]

            video_output_dir = frame_dir
            if save_frames_in_separate_dir:
                # create a separate directory for each video
                if not os.path.exists(os.path.join(frame_dir, video_file_name)):
                    os.makedirs(os.path.join(frame_dir, video_file_name))
                video_output_dir = os.path.join(frame_dir, video_file_name)

            cap = cv2.VideoCapture(video_path)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

            for i in range(0, total_frames, stride):
                if 0 < max_frames <= i:
                    break
                ret, frame = cap.read()
                if ret:
                    cv2.imwrite(os.path.join(video_output_dir, f"{video_file_name}_frame_{i:06d}.png"), frame)
                    num_frames_saved += 1

            cap.release()

    def predict(self, raw_input=None, remove_duplicates=True, sahi=False,
                show_result=False, save_annotated_image=False, annotated_image_file_path='',
                num_frames_to_predict=None):
        if raw_input is None:
            raw_input = self.inference_file

        if annotated_image_file_path == '':
            annotated_image_file_path = self.inference_save_path

        # Make sure the annotated image directory exists
        if not os.path.exists(os.path.basename(annotated_image_file_path)):
            os.makedirs(os.path.basename(annotated_image_file_path), exist_ok=True)

        is_video = False
        # optionally load image
        if isinstance(raw_input, str):
            if raw_input.endswith('.mp4'):
                is_video = True
                result_list = []
                annotated_image_list = []
                cap = cv2.VideoCapture(raw_input)
                total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                frame_number = 0

                if num_frames_to_predict is not None:
                    total_frames = min(total_frames, num_frames_to_predict)

                for i in range(total_frames):
                    ret, frame = cap.read()
                    frame_number += 1
                    if ret:
                        # run inference
                        if sahi:
                            results = self.base_model.sahi_predict(frame)
                        else:
                            results = self.base_model.predict(frame)
                        if remove_duplicates:
                            results = results.with_nms(threshold=self.iou_threshold, class_agnostic=self.class_agnostic)
                        if show_result:
                            plot(
                                    image=frame,
                                    classes=self.base_model.ontology.classes(),
                                    detections=results,
                                    raw=False
                            )

                        annotated_image = plot(
                                image=frame,
                                classes=self.base_model.ontology.classes(),
                                detections=results,
                                raw=True
                        )
                        if save_annotated_image:
                            cv2.imwrite(
                                    f"{annotated_image_file_path}/frame_{frame_number:06d}.png", annotated_image)

                        result_list.append(results)
                        annotated_image_list.append(annotated_image)
                return result_list, annotated_image_list

            else:
                image = cv2.imread(raw_input)

        else:
            image = raw_input

        # run inference
        if sahi:
            results = self.base_model.sahi_predict(image)
        else:
            results = self.base_model.predict(image)

        if remove_duplicates:
            results = results.with_nms(threshold=self.iou_threshold, class_agnostic=self.class_agnostic)
        if show_result:
            plot(
                    image=image,
                    classes=self.base_model.ontology.classes(),
                    detections=results,
                    raw=False
            )

        annotated_image = plot(
                image=image,
                classes=self.base_model.ontology.classes(),
                detections=results,
                raw=True
        )
        if save_annotated_image:
            cv2.imwrite(annotated_image_file_path, annotated_image)

        return results, annotated_image

    def label(self, input_folder: str, extension: str = ".jpg", output_folder: str = None,
              human_in_the_loop: bool = False, roboflow_project: str = None, roboflow_tags: str = ["autodistill"],
              sahi: bool = False,
              record_confidence: bool = False, nms_settings: NmsSetting = NmsSetting.NONE, ) -> sv.DetectionDataset:
        """
        Label a dataset with the model.
        Temporary workaround to avoid out of memory errors.
        """
        if output_folder is None:
            output_folder = input_folder + "_labeled"

        os.makedirs(output_folder, exist_ok=True)

        images_map = {}

        # Create a temporary file for the shelve
        temp_filename = tempfile.mktemp()
        detections_map = shelve.open(temp_filename)

        if sahi:
            slicer = sv.InferenceSlicer(callback=self.base_model.predict)

        files = glob.glob(input_folder + "/*" + extension)
        progress_bar = tqdm(files, desc="Labeling images")
        # iterate through images in input_folder
        for f_path in progress_bar:
            progress_bar.set_description(desc=f"Labeling {f_path}", refresh=True)
            image = cv2.imread(f_path)

            f_path_short = os.path.basename(f_path)
            # images_map[f_path_short] = f_path
            images_map[f_path_short] = image.copy()

            if sahi:
                detections = slicer(image)
            else:
                detections = self.base_model.predict(image)

            if nms_settings == NmsSetting.CLASS_SPECIFIC:
                detections = detections.with_nms()
            if nms_settings == NmsSetting.CLASS_AGNOSTIC:
                detections = detections.with_nms(class_agnostic=self.class_agnostic)

            detections_map[f_path_short] = detections

        detections_map.close()  # Close the shelve file
        detections_map = shelve.open(temp_filename, flag='r')
        dataset = sv.DetectionDataset(
                self.ontology.classes(), images_map, detections_map
        )

        dataset.as_yolo(
                output_folder + "/images",
                output_folder + "/annotations",
                min_image_area_percentage=0.01,
                data_yaml_path=output_folder + "/data.yaml",
        )

        if record_confidence is True:
            self.base_model._record_confidence_in_files(
                    output_folder + "/annotations", images_map, detections_map
            )
        split_data(output_folder, record_confidence=record_confidence)

        if human_in_the_loop:
            roboflow.login()

            rf = roboflow.Roboflow()

            workspace = rf.workspace()

            workspace.upload_dataset(output_folder, project_name=roboflow_project)

        print("Labeled dataset created - ready for distillation.")
        return dataset

    def label_dataset(self, input_folder='', output_folder='', extension='.png', sahi=False):
        if input_folder == '':
            input_folder = self.frame_dir
        if output_folder == '':
            output_folder = self.dataset_dir

        self.base_model.label(
                input_folder=input_folder,
                extension=extension,
                output_folder=output_folder,  # dataset_dir
                record_confidence=True,
                sahi=sahi
        )

    def label_large_dataset(self, batch_size, input_folder='', output_folder='', extension='.png', sahi=False):
        # # iterate through every batch
        # for i in range(0, len(os.listdir(input_folder)), batch_size):
        #     # make a temporary directory and copy batch_size images into it
        #     temp_folder = tempfile.mkdtemp()
        #     shutil.copytree(input_folder, temp_folder)
        #     for i in range(batch_size):
        #         self.label_dataset(input_folder=temp_folder, output_folder=output_folder, extension=extension, sahi=sahi)
        #
        #     self.base_model.label(
        #             input_folder=input_folder,
        #             extension=extension,
        #             output_folder=output_folder,  # dataset_dir
        #             record_confidence=True,
        #             sahi=sahi
        #     )

        # iterate through . todo: use os.scandir to save memory and call next
        files_in_input_folder = os.listdir(input_folder)
        for i in range(0, len(files_in_input_folder), batch_size):
            # copy all files from i to i+batch_size to a temporary directory
            # temp_folder = tempfile.mkdtemp()

            with tempfile.TemporaryDirectory() as temp_folder:
                # temp_folder_path = temp_folder
                for j in range(i, i + batch_size):
                    shutil.copy2(
                            os.path.join(input_folder, files_in_input_folder[j]),
                            temp_folder)
                self.label_dataset(input_folder=temp_folder, output_folder=output_folder, extension=extension,
                                   sahi=sahi)

            # shutil.rmtree(temp_folder)

    def generate_labels(self, threshold=1000, batch_size=1000,
                        input_folder='', output_folder='', extension='.png', sahi=False):
        if input_folder == '':
            input_folder = self.frame_dir
        if output_folder == '':
            output_folder = self.dataset_dir
        if threshold < 1 or threshold is None:
            threshold = self.label_file_size_threshold
        if batch_size < 1 or batch_size is None:
            batch_size = self.label_batch_size

        # check if the number of files in the directory are less than a threshold
        if len(os.listdir(input_folder)) < threshold:
            self.label_dataset(input_folder=input_folder, output_folder=output_folder, extension=extension)
        else:
            # self.label_large_dataset(batch_size,
            #                          input_folder=input_folder, output_folder=output_folder, extension=extension)
            self.label(
                input_folder=input_folder,
                extension=extension,
                output_folder=output_folder,  # dataset_dir
                record_confidence=True,
                sahi=sahi
            )

    def extract_dataset(self, images_directory_path='', annotations_directory_path='', data_yaml_path=''):
        if images_directory_path == '':
            # images_directory_path = self.frame_dir
            images_directory_path = f"{self.dataset_dir}/train/images"
        if annotations_directory_path == '':
            # annotations_directory_path = self.dataset_dir
            annotations_directory_path = f"{self.dataset_dir}/train/labels"
        if data_yaml_path == '':
            data_yaml_path = f"{self.dataset_dir}/data.yaml"

        self.dataset = sv.DetectionDataset.from_yolo(
                images_directory_path=images_directory_path,
                annotations_directory_path=annotations_directory_path,
                data_yaml_path=data_yaml_path
        )

    def plot_labels(self, dataset=None, sample_size=16, sample_grid_size=(4, 4), sample_plot_size=(16, 16)):
        mask_annotator = sv.MaskAnnotator()
        box_annotator = sv.BoxAnnotator()  # sv.BoundingBoxAnnotator()

        if dataset is None:
            if self.dataset is None:
                self.extract_dataset()

            dataset = self.dataset

        # extract the class ids from the dataset dictionary
        class_ids = dataset.classes
        image_names = list(dataset.images.keys())[:sample_size]

        images = []
        for image_name in image_names:
            image = dataset.images[image_name]
            annotations = dataset.annotations[image_name]
            labels = [dataset.classes[class_id] for class_id in annotations.class_id]
            annotated_images = mask_annotator.annotate(scene=image.copy(), detections=annotations)
            annotated_images = box_annotator.annotate(scene=annotated_images, detections=annotations, labels=labels)
            images.append(annotated_images)

        sv.plot_images_grid(
                images=images,
                titles=image_names,
                grid_size=sample_grid_size,
                size=sample_plot_size)

        return images, image_names

    @staticmethod
    def stitch_images_to_video(directory, output_video_path, fps=30, num_frames_to_save=None):
        if num_frames_to_save is None:
            num_frames_to_save = len(os.listdir(directory))

        if num_frames_to_save > len(os.listdir(directory)):
            num_frames_to_save = len(os.listdir(directory))

        # List all image files in the directory with the specified extension
        image_files = [f for f in os.listdir(directory) if f.endswith(".png") or f.endswith(".jpg")]
        image_files.sort()  # Ensure the images are in the correct order

        if len(image_files) == 0:
            print("No image files found in the directory.")
            return None

        # Read the first image to get the frame size
        first_image_path = os.path.join(directory, image_files[0])
        frame = cv2.imread(first_image_path)
        frame_height, frame_width = frame.shape[:2]

        # Make sure the video directory exists
        os.makedirs(os.path.dirname(output_video_path), exist_ok=True)

        # Initialize the video writer
        fourcc = cv2.VideoWriter_fourcc(*"mp4v")  # Codec for .mp4 files
        video_writer = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))

        # Iterate over each image file and write to the video
        for image_file in image_files[:num_frames_to_save]:
            image_path = os.path.join(directory, image_file)
            frame = cv2.imread(image_path)
            video_writer.write(frame)

        # Release the video writer
        video_writer.release()

    def save_inference_video(self, directory, output_video_path, video_name=None, fps=30, num_frames=None):
        # todo: add flag for loading from video or directory
        # get directory from file path
        # output_video_directory = os.path.dirname(output_video_path)
        # todo: use self.plot_labels(sample_size=num_frames) to load detections from the dataset directory
        self.predict(raw_input=video_name, remove_duplicates=self.remove_duplicates, sahi=False,
                     show_result=False, save_annotated_image=True, annotated_image_file_path=directory,
                     num_frames_to_predict=num_frames)
        self.stitch_images_to_video(directory, output_video_path=output_video_path, fps=fps,
                                    num_frames_to_save=num_frames)

    @staticmethod
    def save_caption(dictionary, file_path):
        """
        Saves a dictionary to a text file.

        Args:
            dictionary (dict): The dictionary to save.
            file_path (str): The path to the file where the dictionary will be saved.

        Returns:
            None
        """
        with open(file_path, 'w') as file:
            for key, value in dictionary.items():
                file.write(f"{key}: {value}\n")

    def setup_augmentation_pipeline(self, bounding_box_format='pascal_voc', chained=False):
        """
        Todo: create len(transforms) number of random augmentations to create len(transforms) images instead of one chained image
        :param bounding_box_format: pascal_voc (xmin, ymin, xmax, ymax), coco (x, y, w, h), or yolo (x, y, w, h)
        :return:
        """
        transforms = [
            # A.Resize(512, 512, always_apply=True),
            # A.RandomCrop(width=450, height=450, always_apply=True),
            A.HorizontalFlip(p=0.5),
            A.RandomBrightnessContrast(p=0.2),
            A.OneOf([
                A.RGBShift(),
                A.HueSaturationValue()
            ]),
        ]

        if chained:
            pipeline = A.ReplayCompose(transforms,
                                       bbox_params=A.BboxParams(
                                               format=bounding_box_format,
                                               # min_area=1024,
                                               # min_visibility=0.1,
                                               label_fields=['class_labels', 'class_ids'],
                                               # 'class_labels', 'confidence'
                                       ), p=1.0
                                       )
            self.augmentation_pipeline = [pipeline]

        else:
            self.augmentation_pipeline = []

            for transform in transforms:
                pipeline = A.ReplayCompose([transform],
                                           bbox_params=A.BboxParams(
                                                   format=bounding_box_format,
                                                   # min_area=1024,
                                                   # min_visibility=0.1,
                                                   label_fields=['class_labels', 'class_ids'],
                                                   # 'class_labels', 'confidence'
                                           ), p=1.0
                                           )
                self.augmentation_pipeline.append(pipeline)

    def dataset_augmentation(self, results=None, dataset=None,
                             visualize_augmentations=False, save_augmentations=True,
                             images_directory_path='', annotations_directory_path=''):
        # todo: save the outputs (transformed images, bounding boxes and labels) to the directory
        # todo: plot with bounding boxes
        if images_directory_path == '':
            # images_directory_path = self.frame_dir
            images_directory_path = f"{self.dataset_dir}/train/images"
        if annotations_directory_path == '':
            # annotations_directory_path = self.dataset_dir
            annotations_directory_path = f"{self.dataset_dir}/train/labels"

        if self.mode == 'inference' and results is not None:
            class_labels = [id for id in results.class_id]

            try:
                class_confidence = [id for id in results.confidence]
            except AttributeError as e:
                print("No confidence found")

        if dataset is None:
            if self.dataset is None:
                self.extract_dataset()

            dataset = self.dataset

        if self.save_labels and dataset is not None:
            if self.dataset is None:
                self.extract_dataset()

            class_labels = self.dataset.classes

        image_names = list(dataset.images.keys())

        transformed_images = []

        image = dataset.images[image_names[0]]
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        annotations = dataset.annotations[image_names[0]]
        labels = [dataset.classes[class_id] for class_id in annotations.class_id]
        ids = [class_id for class_id in annotations.class_id]

        replays = []
        for j in range(len(self.augmentation_pipeline)):
            augmentation_pipeline = self.augmentation_pipeline[j]
            data = augmentation_pipeline(
                    image=image,
                    bboxes=annotations.xyxy.tolist(),
                    class_labels=labels,
                    class_ids=ids,
            )
            transformed_image = data['image']
            bboxes = data['bboxes']
            class_labels = data["class_labels"]
            class_ids = data["class_ids"]

            for k in range(len(annotations)):
                area = (bboxes[k][2] - bboxes[k][0]) * (bboxes[k][3] - bboxes[k][1])
                annotations.box_area[k] = area
                annotations.box_area[k] = area

                annotations.xyxy[k] = bboxes[k]
                annotations.class_id[k] = class_ids[k]

            if visualize_augmentations:
                # todo: plot bboxes and labels
                plt.figure(figsize=(10, 10))
                plt.axis('off')
                plt.imshow(image)
                plt.show()

            pipeline_data = data['replay']
            replays.append(pipeline_data)

            transformed_image = cv2.cvtColor(transformed_image, cv2.COLOR_RGB2BGR)
            if save_augmentations:
                # todo: create annotations from transformed coordinates. Get area, box_area and other fields
                # DetectionDataset(
                #         self.base_model.ontology.classes(),
                #         images={image_names[0]: transformed_image},
                #         annotations={image_names[0]: dataset.annotations}).as_yolo(images_directory_path,
                #                                                  annotations_directory_path,
                #                                                  min_image_area_percentage=0.01)
                # save_yolo_annotations(annotations_directory_path, )
                lines = detections_to_yolo_annotations(detections=annotations, image_shape=image.shape,
                                                       min_image_area_percentage=0.01, max_image_area_percentage=1.0,
                                                       approximation_percentage=0.0)
                augmented_file_name = f"{Path(image_names[0]).stem}_augmented_{j}"

                # save the labels
                with open(f"{annotations_directory_path}/{augmented_file_name}.txt", "w") as f:
                    # for line in lines:
                    #     f.write(line + "\n")
                    f.write("\n".join(lines))

                # save the image
                cv2.imwrite(f"{images_directory_path}/{augmented_file_name}.png", transformed_image)


            transformed_images.append(transformed_image)

        # iterate through the images in the dataset
        for i in range(1, len(image_names)):
            image = dataset.images[image_names[i]]
            annotations = dataset.annotations[image_names[i]]
            labels = [dataset.classes[class_id] for class_id in annotations.class_id]

            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

            # apply the augmentation pipeline to the image
            for j in range(len(self.augmentation_pipeline)):
                data_repeated = A.ReplayCompose.replay(
                        replays[j],
                        image=image,
                        bboxes=annotations.xyxy.tolist(),
                        class_labels=labels,
                        class_ids=ids
                )
                transformed_image = data_repeated['image']
                bboxes = data_repeated['bboxes']
                class_labels = data_repeated["class_labels"]
                class_ids = data_repeated["class_ids"]

                for k in range(len(annotations)):
                    area = (bboxes[k][2] - bboxes[k][0]) * (bboxes[k][3] - bboxes[k][1])
                    annotations.box_area[k] = area
                    annotations.box_area[k] = area

                    annotations.xyxy[k] = bboxes[k]
                    annotations.class_id[k] = class_ids[k]

                if visualize_augmentations:
                    # todo: plot bboxes and labels
                    plt.figure(figsize=(10, 10))
                    plt.axis('off')
                    plt.imshow(image)
                    plt.show()

                # pipeline_data = data_repeated['replay']

                transformed_image = cv2.cvtColor(transformed_image, cv2.COLOR_RGB2BGR)
                transformed_images.append(transformed_image)

                if save_augmentations:
                    # save the outputs (transformed images, bounding boxes and labels) to the directory
                    lines = detections_to_yolo_annotations(detections=annotations, image_shape=image.shape,
                                                           min_image_area_percentage=0.01,
                                                           max_image_area_percentage=1.0,
                                                           approximation_percentage=0.0)
                    augmented_file_name = f"{Path(image_names[i]).stem}_augmented_{j}"

                    # save the labels
                    with open(f"{annotations_directory_path}/{augmented_file_name}.txt", "w") as f:
                        # for line in lines:
                        #     f.write(line + "\n")
                        f.write("\n".join(lines))

                    # save the image
                    cv2.imwrite(f"{images_directory_path}/{augmented_file_name}.png", transformed_image)
                    pass

        # todo: save transformed images, bounding boxes and labels to disk
        return transformed_images

    def train_target(self, epochs=50, device=0, resume=False):
        """
        Trains the target model.

        Notes: weights are saved in the './runs/detect/train/weights' folder
        :param epochs:
        :param device:
        :param resume:
        :return:
        """
        self.target_model.train(f"{self.dataset_dir}/data.yaml", epochs=epochs, device=device)

        # self.target_model = YOLO(self.target_model_weights)
        # self.target_model.train(f"{self.dataset_dir}/data.yaml", epochs=epochs, device=device, resume=resume)

    def auto_label(self):
        results, annotated_frame = None, None
        # (optional) extract frames from videos
        if self.extract_frames_from_videos:
            self.extract_frames(stride=1, max_frames=-1)
            if self.interactive_mode:
                print('Extracted frames. Press ENTER to continue.')
                input()

        # (optional) run inference
        if self.mode == 'inference':
            if self.inference_file == '':
                raise ValueError('Inference file must be specified when mode==inference.')
            results, annotated_frame = self.predict(
                    remove_duplicates=self.remove_duplicates, show_result=self.show_label,
                    save_annotated_image=self.save_inference_results)
            if self.interactive_mode:
                print('Inference complete. Press ENTER to continue.')
                input()

        # generate labels
        if self.save_labels:
            self.generate_labels()
            if self.interactive_mode:
                print('Generate labels and created dataset. Press ENTER to continue.')
                input()

        # save captions
        if self.save_captions:
            self.save_caption(self.captions_dict, f"{self.dataset_dir}/captions.txt")
            if self.interactive_mode:
                print('Saved caption. Press ENTER to continue.')
                input()

        if self.save_video:
            self.save_inference_video(self.dataset_dir, output_video_path=f"{self.dataset_dir}/inference.mp4")
            if self.interactive_mode:
                print('Saved inference video. Press ENTER to continue.')
                input()

        # # test extracting the dataset and plotting
        # self.extract_dataset()
        # annotated_images, annotated_image_names = autolabeller.plot_labels()

        # (optional) augment dataset
        if self.augment_dataset:
            self.setup_augmentation_pipeline(bounding_box_format='pascal_voc', chained=False)
            transformed_images = autolabeller.dataset_augmentation(
                    results=results, dataset=None, visualize_augmentations=False, save_augmentations=True)
            if self.interactive_mode:
                print('Augmented dataset. Press ENTER to continue.')
                input()

        # (optional) train target model
        if self.mode == 'training':
            print('Training model.')
            self.train_target(epochs=50, device=0)
            if self.interactive_mode:
                print('Trained model. Press ENTER to continue.')
                input()
        return results, annotated_frame


if __name__ == '__main__':
    base_dir = f'./'
    video_dir = f"{base_dir}/video_dir"
    frame_dir = f"{base_dir}/frames"
    dataset_dir = f"{base_dir}/dataset"
    inference_file = f"{base_dir}/frames/_frame_000000.png"
    inference_save_path = f"{base_dir}/annotations/_annotated_frame_000000.png"

    cli_args = parse_args()

    if 'dino' in cli_args.base_model.lower():
        from autodistill_grounding_dino import GroundingDINO
    elif 'yolo' in cli_args.base_model.lower():
        from autodistill_yolov8 import YOLOv8Base
    elif cli_args.base_model.lower() == 'grounded_sam':
        from autodistill_grounded_sam import GroundedSAM
    elif cli_args.base_model.lower() == 'grounded_sam2':
        from autodistill_grounded_sam_2 import GroundedSAM2
    elif 'florence' in cli_args.base_model.lower():
        from autodistill_florence_2 import Florence2, Florence2Trainer
    elif cli_args.base_model.lower() == 'owlv2':
        from autodistill_owlv2 import OWLv2
    elif 'detic' in cli_args.base_model.lower():
        from autodistill_detic import DETIC
    elif 'codet' in cli_args.base_model.lower():
        from autodistill_codet import CoDet  # automatic setup/installation fails. Will need to build manually like SAM2
    elif cli_args.base_model.lower() == 'owlv2':
        from autodistill_owl_vit import OWLViT  # could debug but not worth the effort
    else:
        from autodistill_grounded_sam import GroundedSAM

    # define an ontology to map class names to our Grounded SAM 2 prompt
    # the ontology dictionary has the format {caption: class}
    # where caption is the prompt sent to the base model, and class is the label that will
    # be saved for that caption in the generated annotations
    # then, load the model
    # different captions can be mapped to the same output, but might lead to duplicates. Make sure to pass remove_duplicates in when prediction or labelling
    captions_dict = {
        "person on wheelchair": "person on wheelchair",
        "cyclist": "cyclist",
        "person on scooter": "person on scooter",
        "person on skateboard": "person on skateboard",
        "person": "person",
        "bicycle": "bicycle",
        "wheelchair": "wheelchair",
        "motorcycle": "motorcycle",
        "car": "car",
        #"vehicle": "vehicle", "vehicle": "car"
        "bus": "bus",
        "truck": "truck",
    }

    ontology = CaptionOntology(captions_dict)

    autolabeller = AutoLabeler(
            mode=cli_args.run_mode,
            base=cli_args.base_model,  # 'grounding_dino', 'grounded_sam', owl_vit, detic, florence_2, 'yolov8', owlv2, codet
            base_model_weights='yolov5su.pt',
            target=cli_args.target_model,  # yolov8, yolov5, yolov6, yolo-nas, yolov7, detr
            target_model_weights=cli_args.target_model_weights,
            captions_dict=captions_dict,  # cli_args.captions_dict. Will implement later
            video_dir=cli_args.video_dir,
            frame_dir=cli_args.frame_dir,
            dataset_dir=cli_args.dataset_dir,
            extract_frames_from_videos=cli_args.extract_frames_from_videos,
            extraction_stride=1,
            extraction_max_frames=-1,
            save_labels=True,
            label_file_size_threshold=1000,
            label_batch_size=1000,
            save_inference_results=cli_args.save_inference_results,
            inference_file=cli_args.inference_file,
            inference_save_path=cli_args.inference_save_path,
            save_video=cli_args.save_auto_labels_video,
            save_captions=cli_args.save_captions,
            show_label=cli_args.show,
            augment_dataset=cli_args.augment_dataset,
            chain_augmentations=False,
            remove_duplicates=cli_args.remove_duplicates,
            iou_threshold=cli_args.iou_threshold,
            class_agnostic=cli_args.class_agnostic,
            interactive_mode=cli_args.interactive_mode
    )

    # # # test updating ontology
    # # new_ontology = CaptionOntology(
    # #                 {"person": "person",
    # #                  "car": "car"}
    # #         )
    # # autolabeller.update_caption(captions_dict)
    #
    # # # test extracting frames
    # # autolabeller.extract_frames(video_dir=None, frame_dir=None, save_frames_in_separate_dir=False,
    # #                             stride=1, max_frames=20)
    #
    # # # test predicting on one image
    # results, annotated_image = autolabeller.predict(raw_input=None, show_result=True, save_annotated_image=True,
    #                                                 annotated_image_file_path='', sahi=False)
    #
    # # test labelling
    # # autolabeller.generate_labels()  # threshold=5, batch_size=5
    # # # autolabeller.label(input_folder=autolabeller.frame_dir, extension=".png", output_folder=autolabeller.dataset_dir)
    #
    # # # test saving captions
    # autolabeller.save_caption(autolabeller.captions_dict,
    #                           f"{autolabeller.dataset_dir}/captions.txt")
    #
    # # test converting labels to videos (test predicting on a video and saving)
    # # autolabeller.save_inference_video(f"{autolabeller.dataset_dir}/annotated_frames",
    # #                                   video_name=f"{video_dir}/VisualCamera4_sample_data_run_2.mp4",
    # #                                   output_video_path=f"{autolabeller.dataset_dir}/inference.mp4",
    # #                                   fps=30, num_frames=10)
    #
    # # test extracting the dataset and plotting
    # autolabeller.extract_dataset()
    # annotated_images, annotated_image_names = autolabeller.plot_labels()
    #
    # # test augmenting the dataset
    # # autolabeller.setup_augmentation_pipeline(bounding_box_format='pascal_voc', chained=True)
    # transformed_images = autolabeller.dataset_augmentation(
    #         results=results, dataset=None, visualize_augmentations=False, save_augmentations=True)
    #
    # # test training the target model
    # autolabeller.train_target(epochs=50)

    # automatically label dataset
    autolabeller.auto_label()

    # # run inference with the target model
    # pred = autolabeller.target_model.predict(f"{autolabeller.dataset_dir}/dataset/valid/your-image.jpg", confidence=0.5)

    print("Done")
